# LoRA finetuning Meta Llama 3.1 on any of your own infra.
# To finetune a 70B model:  
#  sky launch task.yaml -c llama31 --env HF_TOKEN --env WANDB_API_KEY --env MODEL_SIZE=70B

envs:
  MODEL_SIZE: 
  HF_TOKEN:
  DATASET: "yahma/alpaca-cleaned"
  WANDB_API_KEY:

resources:
  cloud: kubernetes
  accelerators: H100:8
num_nodes: 2

# Mount host path to the pod
# https://github.com/nebius/nebius-solution-library/tree/main/k8s-training is mounting 
# the shared filestore to /mnt/data, so we need to mount the same path in the pod
experimental:
  config_overrides:
      kubernetes:
          pod_config:
            spec:
              containers:
                - volumeMounts:
                    - mountPath: /mnt/data
                      name: data-volume
              volumes:
                - name: data-volume
                  hostPath:
                    path: /mnt/data
                    type: Directory

file_mounts:
  /configs: ./configs

setup: |
  pip install torch torchvision torchao torchtune wandb
  tune download meta-llama/Meta-Llama-3.1-${MODEL_SIZE}-Instruct \
    --hf-token $HF_TOKEN \
    --output-dir /tmp/Meta-Llama-3.1-${MODEL_SIZE}-Instruct \
    --ignore-patterns "original/consolidated*"  
  
run: |
  tune run --nproc_per_node $SKYPILOT_NUM_GPUS_PER_NODE \
    lora_finetune_distributed \
    --config /configs/${MODEL_SIZE}-lora.yaml \
    dataset.source=$DATASET
  
  # Save outputs to persistent storage
  mkdir -p /mnt/data/$MODEL_SIZE-lora-output
  rsync -Pavz /tmp/Meta-Llama-3.1-${MODEL_SIZE}-Instruct /mnt/data/$MODEL_SIZE-lora-output
  rm -rf /tmp/Meta-Llama-3.1-${MODEL_SIZE}-Instruct